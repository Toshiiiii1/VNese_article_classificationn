ChatGPT của OpenAI tiêu tốn nửa triệu kWh mỗi ngày, trong khi một gia đình phổ thông tại Mỹ sử dụng trung bình 29 kWh.  New Yorker dẫn tính toán của Alex de Vries, nhà khoa học dữ liệu thuộc Ngân hàng Quốc gia Hà Lan, rằng ChatGPT phải đáp ứng khoảng 200 triệu câu lệnh từ người dùng mỗi ngày, từ đó ngốn nửa triệu kWh. Nếu so với mức điện năng tiêu thụ của một hộ dân Mỹ, năng lượng tiêu thụ của ChatGPT nhiều gấp 17.000 lần và vẫn đang tăng mạnh khi chatbot của OpenAI ngày càng phổ biến. Tương tự ChatGPT, nếu tích hợp AI tạo sinh, công cụ tìm kiếm của Google sẽ tốn 29 tỷ kilowatt giờ mỗi năm. Lượng tiêu thụ điện năng này nhiều hơn cả các quốc gia như Kenya, Guatemala và Croatia. Logo ChatGPT hiển thị trên màn hình điện thoại. Ảnh: AFP Alex de Vries nói với Business Insider rằng AI rất tốn điện, mỗi máy chủ có thể tiêu thụ bằng hơn 10 hộ gia đình ở Anh cộng lại. Tuy nhiên theo The Verge, rất khó ước tính lượng điện ngành AI đang sử dụng trong giai đoạn bùng nổ này, do có sự khác biệt đáng kể trong cách mô hình ngôn ngữ lớn vận hành. Bên cạnh đó, các công ty công nghệ dẫn đầu về AI tạo sinh chưa công bố chính xác về lượng điện họ sử dụng. Trong khi đó, Vries ước tính toàn ngành AI sẽ tiêu thụ từ 85 đến 134 tWh (134 tỷ kWh) mỗi năm. Con số trên được tính toán dựa trên thông số do Nvidia, nhà sản xuất chip hiện chiếm 95% thị phần GPU dành cho AI, công bố. Điện năng tiêu thụ của riêng ngành AI được đánh giá rất lớn khi so với các công ty sử dụng điện nhiều nhất trên thế giới hiện nay, như Samsung cần gần 23 tWh, Google hơn 12 tWh còn Microsoft tiêu thụ 10 tWh để vận hành trung tâm dữ liệu, mạng và thiết bị người dùng. OpenAI chưa bình luận về mức tiêu thụ điện năng khủng của ChatGPT.